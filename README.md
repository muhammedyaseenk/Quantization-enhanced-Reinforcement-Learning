# Quantization-enhanced-Reinforcement-Learning
QeRL is a Quantization-enhanced RL framework for large language models. It combines NVFP4 quantization with LoRA, introduces Adaptive Quantization Noise (AQN) to boost exploration, speeds up rollouts by 1.5Ã—, and enables RL training of 32B models on a single 80GB GPU, matching full fine-tuning on GSM8K and MATH.
